{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4671880",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82807aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "30aec8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6dcdf",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b39053",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c553d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative position encoding\n",
    "\n",
    "class EncoderPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(EncoderPositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(4, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, pos):\n",
    "        pos = self.fc1(pos)\n",
    "        pos = self.relu(pos)\n",
    "        pos = self.fc2(pos)\n",
    "        return x + pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce583909",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23481f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(DecoderPositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4807bbb3",
   "metadata": {},
   "source": [
    "### Multihead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e08a51",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e68c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, max_seq_length):\n",
    "        super(EncoderMultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.max_seq_length = max_seq_length # Maximum sequence length for positional encoding\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "        self.pe = nn.Parameter(torch.randn(self.d_k, max_seq_length)) # Relative position encoding\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, pe, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = (torch.matmul(Q, K.transpose(-2, -1)) + pe) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        pe = torch.matmul(Q, self.pe[:, self.max_seq_length - K.shape[2]:])\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, pe, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a876a",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8bae953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, max_seq_length):\n",
    "        super(DecoderMultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.max_seq_length = max_seq_length # Maximum sequence length for positional encoding\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "        self.pe = nn.Parameter(torch.randn(self.d_k, max_seq_length)) # Relative position encoding\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, pe, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = (torch.matmul(Q, K.transpose(-2, -1)) + pe) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        pe = torch.matmul(Q, self.pe[:, self.max_seq_length - K.shape[2]:])\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, pe, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee181a7b",
   "metadata": {},
   "source": [
    "### Position-wise FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10cdd74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd3d43",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19c779e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout, max_seq_length):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = EncoderMultiHeadAttention(d_model, num_heads, max_seq_length)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8388c",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a09adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout, max_seq_length):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = DecoderMultiHeadAttention(d_model, num_heads, max_seq_length)\n",
    "        self.cross_attn = DecoderMultiHeadAttention(d_model, num_heads, max_seq_length)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6144a41",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d73be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout, max_seq_length) for _ in range(num_encoder_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout, max_seq_length) for _ in range(num_decoder_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, pos, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.encoder_embedding(src))\n",
    "        tgt_embedded = self.dropout(self.decoder_embedding(tgt))\n",
    "\n",
    "        \n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "    \n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397af08",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da374fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "embeddings = datasets.load_from_disk(\"embeddings\")['embeddings']\n",
    "labels = datasets.load_from_disk(\"labels\")['labels']\n",
    "\n",
    "train = datasets.load_from_disk(\"train_dataset\")\n",
    "test = datasets.load_from_disk(\"test_dataset\")\n",
    "validate = datasets.load_from_disk(\"validate_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cef242e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      9\u001b[39m max_seq_length = \u001b[32m200\u001b[39m\n\u001b[32m     10\u001b[39m batch_size = \u001b[32m32\u001b[39m\n\u001b[32m     12\u001b[39m transformer = \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_vocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_vocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_encoder_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_decoder_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43m                          \u001b[49m\u001b[43md_ff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m criterion = nn.CrossEntropyLoss(ignore_index=\u001b[32m0\u001b[39m).to(device)\n\u001b[32m     17\u001b[39m optimizer = optim.Adam(transformer.parameters(), lr=\u001b[32m0.001\u001b[39m, betas=(\u001b[32m0.9\u001b[39m, \u001b[32m0.98\u001b[39m), eps=\u001b[32m1e-9\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda\\envs\\conda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda\\envs\\conda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda\\envs\\conda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda\\envs\\conda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "d_model = 64\n",
    "num_heads = 4\n",
    "num_encoder_layers = 4\n",
    "num_decoder_layers = 4\n",
    "d_ff = 256\n",
    "dropout = 0.1\n",
    "src_vocab_size = len(embeddings)\n",
    "tgt_vocab_size = len(labels) + 2\n",
    "max_seq_length = 200\n",
    "batch_size = 32\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, \n",
    "                          num_heads, num_encoder_layers, num_decoder_layers, \n",
    "                          d_ff, max_seq_length, dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f32441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, maxsize = -1):\n",
    "    emb = data['embeddings']\n",
    "    pos = data['pos']\n",
    "    formula = data['formula']\n",
    "    if maxsize == -1:\n",
    "        maxsize = len(emb)\n",
    "    emb = torch.tensor([[emb[e][i][0] if i < len(emb[e]) else 0 for i in range(max_seq_length)] for e in range(maxsize)])\n",
    "    pos = torch.tensor([[pos[e][i] if i < len(pos[e]) else [0, 0, 0, 0] for i in range(max_seq_length)] for e in range(maxsize)], dtype=torch.float)\n",
    "    formula = torch.tensor([[len(labels) + 1] + formula[i] + [len(labels) + 1] * (max_seq_length - len(formula[i]) - 1) for i in range(maxsize)])\n",
    "    return emb, pos, formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bdab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data, pos_data, tgt_data = create_dataset(train)\n",
    "data_loader = data.DataLoader(data.TensorDataset(src_data, pos_data, tgt_data), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6121b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m     loss = criterion(output.contiguous().view(-\u001b[32m1\u001b[39m, tgt_vocab_size), tgt_data[:, \u001b[32m1\u001b[39m:].contiguous().view(-\u001b[32m1\u001b[39m))\n\u001b[32m     14\u001b[39m     loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda\\envs\\conda-env\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda\\envs\\conda-env\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda\\envs\\conda-env\\Lib\\site-packages\\torch\\optim\\adam.py:218\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;129m@_use_grad_for_differentiable\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    212\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Perform a single optimization step.\u001b[39;00m\n\u001b[32m    213\u001b[39m \n\u001b[32m    214\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[33;03m        closure (Callable, optional): A closure that reevaluates the model\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03m            and returns the loss.\u001b[39;00m\n\u001b[32m    217\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cuda_graph_capture_health_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m     loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda\\envs\\conda-env\\Lib\\site-packages\\torch\\optim\\optimizer.py:436\u001b[39m, in \u001b[36mOptimizer._cuda_graph_capture_health_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_cuda_graph_capture_health_check\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    421\u001b[39m     \u001b[38;5;66;03m# Note [torch.compile x capturable]\u001b[39;00m\n\u001b[32m    422\u001b[39m     \u001b[38;5;66;03m# If we are compiling, we try to take the capturable path automatically by\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    429\u001b[39m     \u001b[38;5;66;03m# Thus, when compiling, inductor will determine if cudagraphs\u001b[39;00m\n\u001b[32m    430\u001b[39m     \u001b[38;5;66;03m# can be enabled based on whether there is input mutation or CPU tensors.\u001b[39;00m\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    432\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m torch.compiler.is_compiling()\n\u001b[32m    433\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m torch.backends.cuda.is_built()\n\u001b[32m    434\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m torch.cuda.is_available()\n\u001b[32m    435\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m         capturing = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_current_stream_capturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m capturing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[32m    439\u001b[39m             group[\u001b[33m\"\u001b[39m\u001b[33mcapturable\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups\n\u001b[32m    440\u001b[39m         ):\n\u001b[32m    441\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    442\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mAttempting CUDA graph capture of step() for an instance of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    443\u001b[39m                 + \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\n\u001b[32m    444\u001b[39m                 + \u001b[33m\"\u001b[39m\u001b[33m but param_groups\u001b[39m\u001b[33m'\u001b[39m\u001b[33m capturable is False.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    445\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda\\envs\\conda-env\\Lib\\site-packages\\torch\\cuda\\graphs.py:30\u001b[39m, in \u001b[36mis_current_stream_capturing\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mis_current_stream_capturing\u001b[39m():\n\u001b[32m     26\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\u001b[39;00m\n\u001b[32m     27\u001b[39m \n\u001b[32m     28\u001b[39m \u001b[33;03m    If a CUDA context does not exist on the current device, returns False without initializing the context.\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cuda_isCurrentStreamCapturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "print('Start training...')\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(1):\n",
    "    for src_data, pos_data, tgt_data in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        src_data = src_data.to(device)\n",
    "        pos_data = pos_data.to(device)\n",
    "        tgt_data = tgt_data.to(device)\n",
    "        \n",
    "        output = transformer(src_data, pos_data, tgt_data[:, :-1])\n",
    "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b7f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 200])\n",
      "tensor([[  0,  42, 234, 515, 282, 282, 431, 297, 374, 515, 157, 515, 150, 431,\n",
      "         431, 515,  95, 164, 431, 468, 343, 515,  61, 374, 515, 220, 431, 515,\n",
      "         217, 431, 431, 251,  95,  61, 374, 515,  95, 431, 515, 468, 282, 431,\n",
      "         344, 199,  42, 234, 515, 356, 431, 297, 468, 343, 515,  61, 374, 515,\n",
      "         220, 431, 515, 217, 431, 431, 157, 515, 150, 431, 251,  95, 208, 374,\n",
      "         515, 490, 431, 515, 468, 282, 431, 344,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0]], device='cuda:0')\n",
      "tensor([], device='cuda:0', size=(0, 200), dtype=torch.int64)\n",
      "torch.Size([1, 199, 580])\n",
      "torch.Size([199, 580])\n",
      "torch.Size([398])\n",
      "tensor([[-2.4977,  1.8370, -2.9126,  ..., -1.8140, -1.3441, -0.1037],\n",
      "        [-2.8085,  0.5109, -3.2958,  ..., -1.6334, -1.9719,  0.7321],\n",
      "        [-3.0244,  0.2219, -3.0908,  ..., -2.2066, -1.6041, -0.8654],\n",
      "        ...,\n",
      "        [-3.0805,  1.4576, -3.4138,  ..., -2.0743, -1.2999, -0.0914],\n",
      "        [-3.0838,  1.4482, -3.6228,  ..., -1.9219, -1.7210,  0.0266],\n",
      "        [-3.1110,  1.2010, -3.8336,  ..., -2.1418, -1.7859, -0.2460]],\n",
      "       device='cuda:0')\n",
      "tensor([ 42, 234, 515,  61, 431, 431, 297, 374, 515, 468, 515, 468, 431, 515,\n",
      "        515, 374, 431, 431, 515, 431, 515, 468, 374, 515, 468, 431, 515, 468,\n",
      "        431, 515, 515, 374, 344, 374, 515, 468, 431, 515, 468, 431, 431, 515,\n",
      "        343, 374, 234, 515, 468, 431, 515, 374, 431, 515, 468, 374, 515, 468,\n",
      "        431, 431, 468, 431, 431, 515, 515, 468, 234, 431, 374, 431, 374, 515,\n",
      "        468, 431, 515,  61, 431, 431, 431, 208, 515, 515,  42, 515, 515,  42,\n",
      "         42, 515, 515,  42,  42,  42, 515,  42,  42,  42,  42,  42, 515, 515,\n",
      "         42,  42, 515,  42,  42, 515,  42, 374,  42, 515, 515, 515, 515,  42,\n",
      "         42,  42, 515,  42,  42,  42,  42, 515,  42,  42,  42,  42, 515,  42,\n",
      "        515, 515, 515, 515, 515, 515,  42, 515,  42,  42,  42, 515,  42, 374,\n",
      "        515,  42, 515,  42,  42,  42,  42, 515, 515, 515, 515,  42,  42, 515,\n",
      "         42,  42, 515, 515, 515, 515,  42, 515,  42,  42, 515,  42,  42,  42,\n",
      "        515,  42, 515, 515, 515,  42,  42,  42,  42,  42,  42,  42,  42,  42,\n",
      "        515, 515,  42, 515, 515,  42,  42,  42, 515,  42,  42,  42, 515, 515,\n",
      "         42,  42, 374])\n",
      "g _ { - } } = \\frac { 2 { 2 } { { \\frac } } { } { 2 \\frac { 2 } { 2 } { { \\frac ) \\frac { 2 } { 2 } } { ^ \\frac _ { 2 } { \\frac } { 2 \\frac { 2 } } 2 } } { { 2 _ } \\frac } \\frac { 2 } { - } } } + { { g { { g g { { g g g { g g g g g { { g g { g g { g \\frac g { { { { g g g { g g g g { g g g g { g { { { { { { g { g g g { g \\frac { g { g g g g { { { { g g { g g { { { { g { g g { g g g { g { { { g g g g g g g g g { { g { { g g g { g g g { { g g \\frac \n",
      "g _ { y y } = \\frac { \\sqrt { Q } } { 1 6 } 2 ^ { - \\frac { 3 } { 4 } } ( 1 - \\frac { 1 } { 2 y } ) , g _ { s } = 2 ^ { - \\frac { 3 } { 4 } } \\sqrt { Q } ( 1 + \\frac { 7 } { 2 y } ) "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(tgt_data.shape)\n",
    "    print(tgt_data[0:1,:-1])\n",
    "    print(tgt_data[0:1][:-1])\n",
    "    output = transformer(src_data[0:1].to(device), pos_data[0:1].to(device), tgt_data[0:1,:-1].to(device)).to(device)\n",
    "    print(output.shape)\n",
    "    print(output.contiguous().view(-1, tgt_vocab_size).shape)\n",
    "    print(tgt_data[0:2, 1:].contiguous().view(-1).shape)\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size).to(device), tgt_data[0:1,1:].contiguous().view(-1).to(device))\n",
    "    output = output[0]\n",
    "    print(output)\n",
    "    ans = []\n",
    "    for i in range(len(output)):\n",
    "        ans.append(torch.argmax(output[i]))\n",
    "    ans = torch.tensor(ans)\n",
    "    print(ans)\n",
    "    \n",
    "    for x in ans:\n",
    "        print(labels[x - 1], end=' ')\n",
    "    print()\n",
    "    \n",
    "    for x in tgt_data[0]:\n",
    "        if x == len(labels) + 1: continue\n",
    "        print(labels[x - 1], end=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
